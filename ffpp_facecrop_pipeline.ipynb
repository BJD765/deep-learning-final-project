{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41f8d5e8",
   "metadata": {},
   "source": [
    "# FaceForensics++ C23 – Face-Crop Pipeline and Training\n",
    "\n",
    "This notebook builds a new pipeline that:\n",
    "\n",
    "1. Loads `FF++_Metadata.csv` and train/val/test CSVs.\n",
    "2. Extracts frames from the original videos and **crops faces** using MTCNN.\n",
    "3. Saves cropped faces to `data_faces/{train,val,test}/{REAL,FAKE}`.\n",
    "4. Trains a ResNet18 classifier on the cropped-face dataset.\n",
    "\n",
    "Use this as an alternative to the full-frame pipeline in `main.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a08d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports and basic paths\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "root = Path(\"FaceForensics++_C23\")\n",
    "print(\"Root:\", root.resolve())\n",
    "\n",
    "# Expect CSVs already created by main.ipynb (selected_videos + train/val/test)\n",
    "print(\"Metadata exists:\", (root / \"csv\" / \"FF++_Metadata.csv\").is_file())\n",
    "print(\"Train CSV exists:\", (root / \"csv\" / \"train_videos.csv\").is_file())\n",
    "print(\"Val CSV exists:\", (root / \"csv\" / \"val_videos.csv\").is_file())\n",
    "print(\"Test CSV exists:\", (root / \"csv\" / \"test_videos.csv\").is_file())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfb477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Install and set up MTCNN face detector\n",
    "# Run this cell once per environment; comment out pip line after install.\n",
    "\n",
    "# !pip install facenet-pytorch\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "mtcnn = MTCNN(keep_all=False, device=device)  # detect a single main face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f4d533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Helper: crop face from a single frame\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def crop_face_from_frame(frame_bgr, margin=0.2, target_size=(224, 224)):\n",
    "    \"\"\"Detect and crop the main face region from a BGR frame.\n",
    "\n",
    "    Returns a BGR image of size target_size, or None if no face is found.\n",
    "    \"\"\"\n",
    "    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    h, w, _ = frame_rgb.shape\n",
    "\n",
    "    boxes, _ = mtcnn.detect(frame_rgb)\n",
    "    if boxes is None or len(boxes) == 0:\n",
    "        return None\n",
    "\n",
    "    # Take the first detected box (or we could choose the largest)\n",
    "    x1, y1, x2, y2 = boxes[0]\n",
    "\n",
    "    # Add a margin around the detected box\n",
    "    bw = x2 - x1\n",
    "    bh = y2 - y1\n",
    "    x1 = max(0, int(x1 - margin * bw))\n",
    "    y1 = max(0, int(y1 - margin * bh))\n",
    "    x2 = min(w, int(x2 + margin * bw))\n",
    "    y2 = min(h, int(y2 + margin * bh))\n",
    "\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None\n",
    "\n",
    "    face_rgb = frame_rgb[y1:y2, x1:x2]\n",
    "    if face_rgb.size == 0:\n",
    "        return None\n",
    "\n",
    "    face_rgb = cv2.resize(face_rgb, target_size)\n",
    "    face_bgr = cv2.cvtColor(face_rgb, cv2.COLOR_RGB2BGR)\n",
    "    return face_bgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509a13a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Frame index sampling (copied from main.ipynb for consistency)\n",
    "\n",
    "def compute_sampled_indices(frame_count, assumed_fps=25, max_frames_per_video=40):\n",
    "    \"\"\"Return list of frame indices to sample given total frame_count.\n",
    "\n",
    "    - step ~ 1 fps -> step = assumed_fps\n",
    "    - start from `step` to avoid very first frames\n",
    "    - cap the number of frames by max_frames_per_video\n",
    "    \"\"\"\n",
    "    if frame_count <= 0:\n",
    "        return []\n",
    "\n",
    "    step = int(assumed_fps)\n",
    "    if step <= 0:\n",
    "        step = 1\n",
    "\n",
    "    start = step  # skip the very first second\n",
    "    indices = list(range(start, frame_count, step))\n",
    "\n",
    "    if len(indices) > max_frames_per_video:\n",
    "        indices = indices[:max_frames_per_video]\n",
    "    return indices\n",
    "\n",
    "\n",
    "# Quick sanity checks\n",
    "for F in [300, 600, 1200]:\n",
    "    idx = compute_sampled_indices(F)\n",
    "    print(f\"F={F}, num_frames={len(idx)}, first={idx[:3]} last={idx[-3:] if idx else []}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be34bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Extract face crops for each split\n",
    "\n",
    "faces_root = Path(\"data_faces\")\n",
    "faces_root.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def extract_face_frames_for_split(\n",
    "    split_name,\n",
    "    csv_name,\n",
    "    max_frames_per_video=40,\n",
    "    assumed_fps=25,\n",
    "    target_size=(224, 224),\n",
    "):\n",
    "    \"\"\"Extract sampled frames for all videos in the given split,\n",
    "    detect and crop face, and save to data_faces/{split}/{REAL|FAKE}.\n",
    "    \"\"\"\n",
    "    split_root = faces_root / split_name\n",
    "    (split_root / \"REAL\").mkdir(parents=True, exist_ok=True)\n",
    "    (split_root / \"FAKE\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(root / \"csv\" / csv_name)\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Extracting {split_name} faces\"):\n",
    "        rel_path = row[\"File Path\"]\n",
    "        label = row[\"Label\"]  # 'REAL' or 'FAKE'\n",
    "        frame_count = int(row.get(\"Frame Count\", -1))\n",
    "\n",
    "        video_path = root / rel_path\n",
    "        if not video_path.is_file():\n",
    "            continue\n",
    "\n",
    "        indices = compute_sampled_indices(\n",
    "            frame_count,\n",
    "            assumed_fps=assumed_fps,\n",
    "            max_frames_per_video=max_frames_per_video,\n",
    "        )\n",
    "        if not indices:\n",
    "            continue\n",
    "\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        if not cap.isOpened():\n",
    "            continue\n",
    "\n",
    "        base_name = video_path.stem\n",
    "\n",
    "        for idx in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "\n",
    "            face = crop_face_from_frame(frame, target_size=target_size)\n",
    "            if face is None:\n",
    "                continue\n",
    "\n",
    "            out_dir = split_root / label\n",
    "            frame_filename = f\"{base_name}_f{idx:05d}.png\"\n",
    "            out_path = out_dir / frame_filename\n",
    "            cv2.imwrite(str(out_path), face)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "\n",
    "print(\"data_faces root:\", faces_root.resolve())\n",
    "print(\"To run extraction, call extract_face_frames_for_split for train/val/test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7f02a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Run extraction (WARNING: long-running)\n",
    "\n",
    "# Uncomment and run one by one when ready (after Kaggle download is complete).\n",
    "# extract_face_frames_for_split(\"train\", \"train_videos.csv\")\n",
    "# extract_face_frames_for_split(\"val\", \"val_videos.csv\")\n",
    "# extract_face_frames_for_split(\"test\", \"test_videos.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97acee0e",
   "metadata": {},
   "source": [
    "## Load cropped-face dataset and train ResNet18\n",
    "\n",
    "After running the extraction above, `data_faces/` will contain:\n",
    "\n",
    "- `data_faces/train/REAL` and `data_faces/train/FAKE`\n",
    "- `data_faces/val/REAL` and `data_faces/val/FAKE`\n",
    "- `data_faces/test/REAL` and `data_faces/test/FAKE`\n",
    "\n",
    "We can now reuse the same training code structure as in `main.ipynb`, just pointing to `data_faces` instead of `data_frames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923da64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. ImageFolder datasets and DataLoaders for data_faces\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "faces_dir = Path(\"data_faces\")\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.08),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "])\n",
    "\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=str(faces_dir / \"train\"), transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder(root=str(faces_dir / \"val\"), transform=eval_transform)\n",
    "test_dataset = datasets.ImageFolder(root=str(faces_dir / \"test\"), transform=eval_transform)\n",
    "\n",
    "print(\"Classes:\", train_dataset.classes)\n",
    "print(\"Train images:\", len(train_dataset))\n",
    "print(\"Val images:\", len(val_dataset))\n",
    "print(\"Test images:\", len(test_dataset))\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "len(train_loader), len(val_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4e2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Define ResNet18 model, loss, optimizer (similar to main.ipynb)\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "# If you want to reuse class_weights from main.ipynb, you can paste them here manually.\n",
    "# For simplicity, we start with unweighted loss; you can add weights later if needed.\n",
    "\n",
    "cw = None\n",
    "\n",
    "\n",
    "def create_model(num_classes=2, use_pretrained=True):\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if use_pretrained else None)\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(p=0.3),\n",
    "        nn.Linear(in_features, num_classes),\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model(num_classes=2, use_pretrained=True).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=cw)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "print(model.fc)\n",
    "criterion, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b2ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Training and validation loop (copied from main.ipynb)\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "        total += inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = running_corrects / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "        total += inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = running_corrects / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "num_epochs = 20\n",
    "best_val_acc = 0.0\n",
    "best_state_dict = None\n",
    "\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, \"\n",
    "        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}, \"\n",
    "        f\"time={elapsed:.1f}s\"\n",
    "    )\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state_dict = model.state_dict()\n",
    "\n",
    "print(\"Best val acc:\", best_val_acc)\n",
    "\n",
    "if best_state_dict is not None:\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    torch.save(model.state_dict(), \"best_resnet_ffpp_faces_binary.pth\")\n",
    "    print(\"Saved best model to best_resnet_ffpp_faces_binary.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17144961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Test evaluation and plots\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_simple(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += torch.sum(preds == labels).item()\n",
    "        total += inputs.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "test_acc = evaluate_simple(model, test_loader, device)\n",
    "print(\"Test accuracy (faces):\", test_acc)\n",
    "\n",
    "# Plot training and validation loss/accuracy\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over epochs (faces)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history[\"train_acc\"], label=\"Train Acc\")\n",
    "plt.plot(history[\"val_acc\"], label=\"Val Acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy over epochs (faces)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_predictions_and_labels(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return all_preds, all_labels\n",
    "\n",
    "\n",
    "y_pred, y_true = get_predictions_and_labels(model, test_loader, device)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=train_dataset.classes, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1b4b04",
   "metadata": {},
   "source": [
    "## Day Experiment – Face Crops from Existing `data_frames`\n",
    "\n",
    "This section builds a quicker experiment:\n",
    "\n",
    "- Input: already preprocessed full-frame images in `data_frames/{train,val,test}/{REAL,FAKE}`.\n",
    "- Step: run MTCNN on each 224×224 image, crop face, and save to `data_faces_from_frames/...`.\n",
    "- Then: train the same ResNet18 model on this new dataset to see if overfitting improves.\n",
    "\n",
    "Use this to get faster feedback during the day before running the full raw-video face-crop pipeline overnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e3c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Paths for existing frames and new face-crops-from-frames\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Existing full-frame images from main.ipynb\n",
    "frames_root = Path(\"data_frames\")  # already contains train/val/test/REAL,FAKE\n",
    "print(\"Existing frames root:\", frames_root.resolve())\n",
    "\n",
    "faces_from_frames_root = Path(\"data_faces_from_frames\")\n",
    "faces_from_frames_root.mkdir(exist_ok=True)\n",
    "print(\"Faces-from-frames root:\", faces_from_frames_root.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee700e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Helper: crop faces from a folder of existing 224x224 frames\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def crop_faces_in_split_from_frames(split_name, max_files_per_class=None):\n",
    "    \"\"\"For a given split (train/val/test),\n",
    "    read images from data_frames/{split}/{REAL,FAKE},\n",
    "    run MTCNN to detect/crop face, and save to\n",
    "    data_faces_from_frames/{split}/{REAL,FAKE}.\n",
    "\n",
    "    max_files_per_class: optional cap on number of images per class\n",
    "    (for quicker experiments).\n",
    "    \"\"\"\n",
    "    src_split = frames_root / split_name\n",
    "    dst_split = faces_from_frames_root / split_name\n",
    "\n",
    "    for cls in [\"REAL\", \"FAKE\"]:\n",
    "        src_dir = src_split / cls\n",
    "        dst_dir = dst_split / cls\n",
    "        dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if not src_dir.exists():\n",
    "            print(f\"Warning: source dir not found: {src_dir}\")\n",
    "            continue\n",
    "\n",
    "        files = [p for p in src_dir.iterdir() if p.is_file()]\n",
    "        if max_files_per_class is not None:\n",
    "            files = files[: max_files_per_class]\n",
    "\n",
    "        print(f\"[{split_name}] {cls}: {len(files)} images to process\")\n",
    "\n",
    "        for img_path in tqdm(files, desc=f\"{split_name}-{cls}\"):\n",
    "            frame_bgr = cv2.imread(str(img_path))\n",
    "            if frame_bgr is None:\n",
    "                continue\n",
    "\n",
    "            face_bgr = crop_face_from_frame(frame_bgr, target_size=(224, 224))\n",
    "            if face_bgr is None:\n",
    "                continue\n",
    "\n",
    "            out_path = dst_dir / img_path.name\n",
    "            cv2.imwrite(str(out_path), face_bgr)\n",
    "\n",
    "    print(f\"Done processing split: {split_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1291430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Run face-cropping from existing frames (quick experiment)\n",
    "\n",
    "# You can set max_files_per_class to a smaller number (e.g., 200)\n",
    "# for a very fast test, or None to process all images in each class.\n",
    "\n",
    "# Example: quick test on 200 images per class\n",
    "# crop_faces_in_split_from_frames(\"train\", max_files_per_class=200)\n",
    "# crop_faces_in_split_from_frames(\"val\", max_files_per_class=200)\n",
    "# crop_faces_in_split_from_frames(\"test\", max_files_per_class=200)\n",
    "\n",
    "# Example: full run on all available images (may take longer)\n",
    "# crop_faces_in_split_from_frames(\"train\", max_files_per_class=None)\n",
    "# crop_faces_in_split_from_frames(\"val\", max_files_per_class=None)\n",
    "# crop_faces_in_split_from_frames(\"test\", max_files_per_class=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f07b5d",
   "metadata": {},
   "source": [
    "### Train on `data_faces_from_frames` (day experiment)\n",
    "\n",
    "After running the cropping above, the quick-experiment dataset will be in:\n",
    "\n",
    "- `data_faces_from_frames/train/{REAL,FAKE}`\n",
    "- `data_faces_from_frames/val/{REAL,FAKE}`\n",
    "- `data_faces_from_frames/test/{REAL,FAKE}`\n",
    "\n",
    "We can reuse the same training code structure, just pointing to this new root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c47246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. DataLoaders for data_faces_from_frames (quick experiment)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device (day experiment):\", device)\n",
    "\n",
    "faces_ff_root = Path(\"data_faces_from_frames\")\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform_ff = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.08),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "])\n",
    "\n",
    "\n",
    "eval_transform_ff = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset_ff = datasets.ImageFolder(root=str(faces_ff_root / \"train\"), transform=train_transform_ff)\n",
    "val_dataset_ff = datasets.ImageFolder(root=str(faces_ff_root / \"val\"), transform=eval_transform_ff)\n",
    "test_dataset_ff = datasets.ImageFolder(root=str(faces_ff_root / \"test\"), transform=eval_transform_ff)\n",
    "\n",
    "print(\"[from_frames] Classes:\", train_dataset_ff.classes)\n",
    "print(\"[from_frames] Train images:\", len(train_dataset_ff))\n",
    "print(\"[from_frames] Val images:\", len(val_dataset_ff))\n",
    "print(\"[from_frames] Test images:\", len(test_dataset_ff))\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader_ff = DataLoader(train_dataset_ff, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader_ff = DataLoader(val_dataset_ff, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader_ff = DataLoader(test_dataset_ff, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "len(train_loader_ff), len(val_loader_ff), len(test_loader_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a0f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Model and training for data_faces_from_frames (reuse same architecture)\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import time\n",
    "\n",
    "\n",
    "cw_ff = None  # can plug class weights here if desired\n",
    "\n",
    "\n",
    "def create_model_ff(num_classes=2, use_pretrained=True):\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if use_pretrained else None)\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(p=0.3),\n",
    "        nn.Linear(in_features, num_classes),\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_ff = create_model_ff(num_classes=2, use_pretrained=True).to(device)\n",
    "\n",
    "criterion_ff = nn.CrossEntropyLoss(weight=cw_ff)\n",
    "optimizer_ff = torch.optim.Adam(model_ff.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler_ff = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_ff, mode=\"min\", factor=0.5, patience=1)\n",
    "\n",
    "\n",
    "def train_one_epoch_ff(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "        total += inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = running_corrects / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_ff(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "        total += inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = running_corrects / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "num_epochs_ff = 15\n",
    "best_val_acc_ff = 0.0\n",
    "best_state_dict_ff = None\n",
    "\n",
    "history_ff = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "for epoch in range(num_epochs_ff):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_one_epoch_ff(model_ff, train_loader_ff, optimizer_ff, criterion_ff, device)\n",
    "    val_loss, val_acc = evaluate_ff(model_ff, val_loader_ff, criterion_ff, device)\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    history_ff[\"train_loss\"].append(train_loss)\n",
    "    history_ff[\"train_acc\"].append(train_acc)\n",
    "    history_ff[\"val_loss\"].append(val_loss)\n",
    "    history_ff[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    scheduler_ff.step(val_loss)\n",
    "\n",
    "    print(\n",
    "        f\"[from_frames] Epoch {epoch+1}/{num_epochs_ff} | \"\n",
    "        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, \"\n",
    "        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}, \"\n",
    "        f\"time={elapsed:.1f}s\"\n",
    "    )\n",
    "\n",
    "    if val_acc > best_val_acc_ff:\n",
    "        best_val_acc_ff = val_acc\n",
    "        best_state_dict_ff = model_ff.state_dict()\n",
    "\n",
    "print(\"[from_frames] Best val acc:\", best_val_acc_ff)\n",
    "\n",
    "if best_state_dict_ff is not None:\n",
    "    model_ff.load_state_dict(best_state_dict_ff)\n",
    "    torch.save(model_ff.state_dict(), \"best_resnet_ffpp_faces_from_frames.pth\")\n",
    "    print(\"Saved best model to best_resnet_ffpp_faces_from_frames.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c7cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Evaluation and plots for data_faces_from_frames (day experiment)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_simple_ff(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += torch.sum(preds == labels).item()\n",
    "        total += inputs.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "test_acc_ff = evaluate_simple_ff(model_ff, test_loader_ff, device)\n",
    "print(\"[from_frames] Test accuracy:\", test_acc_ff)\n",
    "\n",
    "# Plot training and validation loss/accuracy\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_ff[\"train_loss\"], label=\"Train Loss\")\n",
    "plt.plot(history_ff[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over epochs (faces_from_frames)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_ff[\"train_acc\"], label=\"Train Acc\")\n",
    "plt.plot(history_ff[\"val_acc\"], label=\"Val Acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy over epochs (faces_from_frames)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_predictions_and_labels_ff(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return all_preds, all_labels\n",
    "\n",
    "\n",
    "y_pred_ff, y_true_ff = get_predictions_and_labels_ff(model_ff, test_loader_ff, device)\n",
    "\n",
    "cm_ff = confusion_matrix(y_true_ff, y_pred_ff)\n",
    "print(\"[from_frames] Confusion Matrix (rows=true, cols=pred):\")\n",
    "print(cm_ff)\n",
    "\n",
    "print(\"\\n[from_frames] Classification Report:\")\n",
    "print(classification_report(y_true_ff, y_pred_ff, target_names=train_dataset_ff.classes, digits=4))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
